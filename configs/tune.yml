# tune.yml

mode: "tune"
project_name: "deep-stylometry"
group_name: "tune-deepstylometry-512-se-default"

data:
  ds_name: "se" # or halvest
  batch_size: 16
  tokenizer_name: "FacebookAI/roberta-base"
  max_length: 512
  map_batch_size: 1000
  load_from_cache_file: true
  # config_name: null
  mlm_collator: false # only if you use an encoder

model:
  base_model_name: "FacebookAI/roberta-base"
  is_decoder_model: false
  add_linear_layers: true
  dropout: 0.1
  contrastive_temp:
    type: uniform
    min: 0.01
    max: 1.0
  pooling_method: "li" # li or mean
  # --- late interaction
  distance_weightning:
    type: choice
    values: ["none", "exp", "linear"]
  alpha:
    type: loguniform
    min: 0.1
    max: 1.0
  use_softmax: true
  initial_gumbel_temp: 1.0
  auto_anneal_gumbel: true
  min_gumbel_temp: 0.5

tune:
  loss: "info_nce"
  margin: null # Only if using triplet loss
  lm_loss_weight: 0.0
  # --- optimizer ---
  lr:
    type: loguniform
    min: 9.0e-6
    max: 1.0e-4
  betas:
    type: choice
    values: [[0.9, 0.999], [0.8, 0.999], [0.7, 0.999]]
  eps:
    type: choice
    values: [1.0e-7, 1.0e-8, 1.0e-9]
  weight_decay:
    type: loguniform
    min: 0.01
    max: 0.2
  num_cycles: 0.5
  # --- trainer ---
  device: "gpu"
  num_devices_per_trial: 1
  num_cpus_per_trial: 10
  max_epochs: 3
  log_every_n_steps: 1
  accumulate_grad_batches: 8
  gradient_clip_val:
    type: choice
    values: [null, 1.0, 2.0, 5.0]
  precision: "32" # 32-true or bf16-mixed
  # --- tuner ---
  metric: "val_total_loss"
  mode: "min"
  num_samples: 30
  max_concurrent_trials: 3
  time_budget_s: 151200 # 42h (in seconds)
  max_t: 3
  grace_period: 1
  # --- wandb logger ---
  use_wandb: true
